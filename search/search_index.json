{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"my_profile/","text":"Profile \u00b6 I'm a software engineer and sometimes machine learning engineer. Some of my projects can be the basis for ai wife achievement. Twitter account is here . I need support for my projects to create my software (mostly cloud GPU fee and real GPU fee) and test new sensors and actuators. Please help me :D on my github sponsor account .","title":"Profile"},{"location":"my_profile/#profile","text":"I'm a software engineer and sometimes machine learning engineer. Some of my projects can be the basis for ai wife achievement. Twitter account is here . I need support for my projects to create my software (mostly cloud GPU fee and real GPU fee) and test new sensors and actuators. Please help me :D on my github sponsor account .","title":"Profile"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/","text":"The brief survey of the 3D video reconstruction for Looking Glass \u00b6 In this article, I will explain overview of the methods to create 3D Videos for Looking Glass. This article is for the people who want to create apps to create 3D Videos and not for app users. The target video is any kind of video, for a video where multiple people is moving as a foreground and foreground is fixed, the method I implemented in remimi can be used. I will explain the method in another article. Firstly I will explain video format we want to output, and secondly the methods that is useful to output the video. 3D Video Format we adopt \u00b6 Before explaining video format, I explain apps available. There are some apps for Looking Glass. To play 3D Videos on Looking Glass, you can use HoloPlay Studio. The supported video formats are Quilt Video RGBD Video The format is really simple, quilt video contains images rfom all the views per frame and RGBD video contains a rgb image and a depth image per frame. The software only supports windows and mac. For ubuntu, you need to write your quilt video player. I recommend using Unity Plugin because it mostly requires just clicking and few types. To record these videos, there's a DepthRecorder . This only supports azure kinect on Windows, but you can record RGBD video using this software. The quilt video format allow us to put more algorithm, meaning there's more room to improvement. So I chose quilt video generation for it. The research survey for quilt video generation \u00b6 Quilt video contains quilt image per frame. Quilt image is a collection of images from views of horizontally different angles. Thus to generate this quilt image, we need to prepare images from horizontally different views. I surveyed researches for the sensor configurations that can be constructed within $1000 USD cost. End-to-End View Synthesis from single or two rgb image \u00b6 There are some researches that synthesis novel view images from a single view or two views (stereo camera) in a end to end manner. Google's models output a MPI image and from the MPI image novel view image is synthesized. The MPI image is lightweight expression of a scene of thin layer and can be used to render novel view images fast. There are methods for stereo images and a single image . The two images below are the network structure for stereo images and a single image each. These networks output MPI images and novel view images are rendered from MPI images. Facebook's Synsin approach predict a rgb image given a rgb image and a relative pose to the input rgb image. The model is depicted in the image below. All of the model is trained on limited dataset (few number of indoor and outdoor dataset), but the facebook's Synsin model is more generalizable than other models from the evaluation result of training the model on outdoor dataset and test on indoor dataset. These are the images from the facebook's animation. Dis-occluded region is dirty and apparent. Look at the original GIF for check. These hallucinated regions are a bit dirtier than the single inpainting methods, but it's not a big difference when applying to video, because they both don't have a consistency between video frames. I guess that this model learn's something like how to inpaint the dis-occluded region from the viewpoint change. The demo of the facebook model seems not perfect, but it might be used to video on narrower HFOV 3D monitors like Looking Glass. Also this is a good starting point for the 3D video generator for any videos. Hand crafted view synthesis from single RGB or RGBD image \u00b6 Hand crafted way is firstly convert into point cloud and render point cloud from different views for quilt image generation. It is easy to create quilt image from RGBD because there's already depth channel and point cloud can be reconstructed. We can just generate quilt video from the point cloud by moving virtual camera horizontally. For an RGB image, the Depth channel can be estimated from monocular depth estimation methods like DPT. For this approach, one of the two primary problem we want to solve is inpainting of dis-occluded region and boundary problem. Inpainting of dis-occluded region \u00b6 There are two ways of inpainting, inpainting depth channel and rgb channels separately or jointly. In \"3D Ken Burns Effect from a Single Image\" , they constructed two networks. One is the network to extract context region used for inpaint, and the other network is inpainting network that output inpainted RGBD image given RGBD image and context vector. In \"3D Photography using Context-aware Layered Depth Inpainting\" , the inpainting process has two stages as you can see in the figure below.Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. The reason might be that the inpainting model utilize edge in the second method and it preserves edge better. Firstly edge image is inpainted and inpainted edge is given to to the second network along with RGB channel and depth channel. Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. Either of these inpainting method is the method for single image and applying to video might cause flickering of depth and color channel. It's ok for a single image 3d reconstruction, but it might not be enough for 3d video generation. Boundary of the objects \u00b6 Another problem is boundary, with most of the depth estimation method, boundary is not clear. Also boundary is very sparse and there's continuity between background and objects. These point looks garbages when seeing 3d monitors. One of the solution is simple filters, but it doesn't fully solve the problem and another solution will be the reconstruction based on the knowledge of objects. For example, there's many way to reconstruct an human 3d mesh from single RGB image using the knowledge of the human shape. For example, PiFU learns human shape and reconstruct an high resolution human mesh from a single RGB image. SMPL is a parameterized body shape constructed from data, and often used by the model to extract body mesh shape from a single RGB image. Currently it is difficult to model the clothed people, and one of the successful method is SCANimate . It seems that there's still need to be advances for handling boundary of the objects. Conclusion \u00b6 Currently Synsin seems the balanced and well performing method to synthesize novel views from wide range of images and videos. For a better quality, I might take either of ways. Increase data for synsin and train them more. (more than 3-6 days for one training.) Try handcrafted way using multiple models. Either of it seems difficult, but I will try after implementing Synsin.","title":"The brief survey of the 3D video reconstruction for Looking Glass"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#the-brief-survey-of-the-3d-video-reconstruction-for-looking-glass","text":"In this article, I will explain overview of the methods to create 3D Videos for Looking Glass. This article is for the people who want to create apps to create 3D Videos and not for app users. The target video is any kind of video, for a video where multiple people is moving as a foreground and foreground is fixed, the method I implemented in remimi can be used. I will explain the method in another article. Firstly I will explain video format we want to output, and secondly the methods that is useful to output the video.","title":"The brief survey of the 3D video reconstruction for Looking Glass"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#3d-video-format-we-adopt","text":"Before explaining video format, I explain apps available. There are some apps for Looking Glass. To play 3D Videos on Looking Glass, you can use HoloPlay Studio. The supported video formats are Quilt Video RGBD Video The format is really simple, quilt video contains images rfom all the views per frame and RGBD video contains a rgb image and a depth image per frame. The software only supports windows and mac. For ubuntu, you need to write your quilt video player. I recommend using Unity Plugin because it mostly requires just clicking and few types. To record these videos, there's a DepthRecorder . This only supports azure kinect on Windows, but you can record RGBD video using this software. The quilt video format allow us to put more algorithm, meaning there's more room to improvement. So I chose quilt video generation for it.","title":"3D Video Format we adopt"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#the-research-survey-for-quilt-video-generation","text":"Quilt video contains quilt image per frame. Quilt image is a collection of images from views of horizontally different angles. Thus to generate this quilt image, we need to prepare images from horizontally different views. I surveyed researches for the sensor configurations that can be constructed within $1000 USD cost.","title":"The research survey for quilt video generation"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#end-to-end-view-synthesis-from-single-or-two-rgb-image","text":"There are some researches that synthesis novel view images from a single view or two views (stereo camera) in a end to end manner. Google's models output a MPI image and from the MPI image novel view image is synthesized. The MPI image is lightweight expression of a scene of thin layer and can be used to render novel view images fast. There are methods for stereo images and a single image . The two images below are the network structure for stereo images and a single image each. These networks output MPI images and novel view images are rendered from MPI images. Facebook's Synsin approach predict a rgb image given a rgb image and a relative pose to the input rgb image. The model is depicted in the image below. All of the model is trained on limited dataset (few number of indoor and outdoor dataset), but the facebook's Synsin model is more generalizable than other models from the evaluation result of training the model on outdoor dataset and test on indoor dataset. These are the images from the facebook's animation. Dis-occluded region is dirty and apparent. Look at the original GIF for check. These hallucinated regions are a bit dirtier than the single inpainting methods, but it's not a big difference when applying to video, because they both don't have a consistency between video frames. I guess that this model learn's something like how to inpaint the dis-occluded region from the viewpoint change. The demo of the facebook model seems not perfect, but it might be used to video on narrower HFOV 3D monitors like Looking Glass. Also this is a good starting point for the 3D video generator for any videos.","title":"End-to-End View Synthesis from single or two rgb image"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#hand-crafted-view-synthesis-from-single-rgb-or-rgbd-image","text":"Hand crafted way is firstly convert into point cloud and render point cloud from different views for quilt image generation. It is easy to create quilt image from RGBD because there's already depth channel and point cloud can be reconstructed. We can just generate quilt video from the point cloud by moving virtual camera horizontally. For an RGB image, the Depth channel can be estimated from monocular depth estimation methods like DPT. For this approach, one of the two primary problem we want to solve is inpainting of dis-occluded region and boundary problem.","title":"Hand crafted view synthesis from single RGB or RGBD image"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#inpainting-of-dis-occluded-region","text":"There are two ways of inpainting, inpainting depth channel and rgb channels separately or jointly. In \"3D Ken Burns Effect from a Single Image\" , they constructed two networks. One is the network to extract context region used for inpaint, and the other network is inpainting network that output inpainted RGBD image given RGBD image and context vector. In \"3D Photography using Context-aware Layered Depth Inpainting\" , the inpainting process has two stages as you can see in the figure below.Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. The reason might be that the inpainting model utilize edge in the second method and it preserves edge better. Firstly edge image is inpainted and inpainted edge is given to to the second network along with RGB channel and depth channel. Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. Either of these inpainting method is the method for single image and applying to video might cause flickering of depth and color channel. It's ok for a single image 3d reconstruction, but it might not be enough for 3d video generation.","title":"Inpainting of dis-occluded region"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#boundary-of-the-objects","text":"Another problem is boundary, with most of the depth estimation method, boundary is not clear. Also boundary is very sparse and there's continuity between background and objects. These point looks garbages when seeing 3d monitors. One of the solution is simple filters, but it doesn't fully solve the problem and another solution will be the reconstruction based on the knowledge of objects. For example, there's many way to reconstruct an human 3d mesh from single RGB image using the knowledge of the human shape. For example, PiFU learns human shape and reconstruct an high resolution human mesh from a single RGB image. SMPL is a parameterized body shape constructed from data, and often used by the model to extract body mesh shape from a single RGB image. Currently it is difficult to model the clothed people, and one of the successful method is SCANimate . It seems that there's still need to be advances for handling boundary of the objects.","title":"Boundary of the objects"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#conclusion","text":"Currently Synsin seems the balanced and well performing method to synthesize novel views from wide range of images and videos. For a better quality, I might take either of ways. Increase data for synsin and train them more. (more than 3-6 days for one training.) Try handcrafted way using multiple models. Either of it seems difficult, but I will try after implementing Synsin.","title":"Conclusion"},{"location":"3d_reconstruction/02_try_syinsin/","text":"\u00b6 Takeaway \u00b6 The synsin can be a nice baseline for the novel view synthesis task if it's not for realtime novel view synthesis.","title":"02 try syinsin"},{"location":"3d_reconstruction/02_try_syinsin/#_1","text":"","title":""},{"location":"3d_reconstruction/02_try_syinsin/#takeaway","text":"The synsin can be a nice baseline for the novel view synthesis task if it's not for realtime novel view synthesis.","title":"Takeaway"},{"location":"dataset/01_why_annotation/","text":"why dataset construction article? \u00b6 The dataset construction is important to train a good ML model. Both the quality and the quantity is really important for the dataset. The mediapipe's hand keypoint detection model is trained with more than 100000 synthetic CG images and real images. But it's still not perfect and if you need more performance you need to collect more data with high quality annotation. Also the image below shows that the better quality data result in a better performance model. (from this slid e). Achieving both quality and quantity is sometimes difficult for some tasks. To increase the number of the annotated data, it will take time linearly increased to the number of annotated data. If the number of the data is increased, the quality control of the annotation will take more time. So both achieving quality and the quantity costs a lot. Thus if your responsibility is getting a good model, it is worth taking time to learn how to construct an dataset efficiently and with a good quality for various tasks. For this purpose I wrote an article about dataset construction for each task from research and industries. In my articles, I will try to focus on methods that meet one of these conditions the ones that can enable something that can be done without the method the ones that can let us construct dataset 2-100 times more efficient the ones that can be used widespread in many tasks (e.g. not a UI improvement for a specific tasks)","title":"why dataset construction article?"},{"location":"dataset/01_why_annotation/#why-dataset-construction-article","text":"The dataset construction is important to train a good ML model. Both the quality and the quantity is really important for the dataset. The mediapipe's hand keypoint detection model is trained with more than 100000 synthetic CG images and real images. But it's still not perfect and if you need more performance you need to collect more data with high quality annotation. Also the image below shows that the better quality data result in a better performance model. (from this slid e). Achieving both quality and quantity is sometimes difficult for some tasks. To increase the number of the annotated data, it will take time linearly increased to the number of annotated data. If the number of the data is increased, the quality control of the annotation will take more time. So both achieving quality and the quantity costs a lot. Thus if your responsibility is getting a good model, it is worth taking time to learn how to construct an dataset efficiently and with a good quality for various tasks. For this purpose I wrote an article about dataset construction for each task from research and industries. In my articles, I will try to focus on methods that meet one of these conditions the ones that can enable something that can be done without the method the ones that can let us construct dataset 2-100 times more efficient the ones that can be used widespread in many tasks (e.g. not a UI improvement for a specific tasks)","title":"why dataset construction article?"},{"location":"dataset/02_instance_segmentation_annotation/","text":"How I annotated 300 images with instance segmentation mask \u00b6 This is an article about how I annotate hand dataset fast. Firstly I'll explain annotation data formats and what I chose because this is important to choose the annotation tol. Annotation data formats \u00b6 There are several segmentation tasks and which format should we choose? Panoptic segmentation format of the coco dataset seems flexible because the panoptic dataset can be used to train instance segmentation, semantic segmentation, panoptic segmentation. Briefly, the data format of the panoptic annotation is png image and json file. Each pixel of the image contains segment ids and the json file contains class_id and other metadata of the segments. The detail of the data format is there . Unfortunately there's no annotation tools that supports the panoptic annotation format. So we need other options. One of the other options is VOC format . That is supported by many tools. The png image has a class id and there's no distinction between different instances. Other option is instance segmentation format. Coco format can contain segment information for each objects and we can utilize it. I think the best format depends on task to solve. For some case, if the target scene to segment only contains stuff, we can choose semantic segmentation and the target scenes to segment only contain things we can choose instance segmentation format. You can find the informal explanation of things/stuff here . Tools \u00b6 There are some tools that can be used to segmentation tasks. Unfortunately AFAIK, there's no tools that can output panoptic coco format. So currently we need to choose an annotation tool that supports both coco instance segmentation format and voc segmentation format. Labelme and CVAT both supports both formats. There's a brief description of these tools. Labelme \u00b6 Labelme is really simple tool that support folder based data/task management and supports coco like format for instance segmentation and VOC like for semantic segmentation. This is all written in python and it can be easily modified by software engineer in machine learning. CVAT \u00b6 CVAT is annotation + data/annotation task management platform. This is really nice for larger scale team. This tool is It is written in Typescript and need someone who knows Typescript. It has a auto/semi automatic annotation tools that can be potentially helpful and it can be extensible. About dataset, it can export both VOC like format and COCO format. Example: Hand Annotation Task \u00b6 For a hand annotation task, exporting a file as coco format is suitable, because human body are things (not stuff) and it is better to treat them as a instance segmentation. This is natural because when imagining image editing application, we want to distinguish And for my case, I wanted to use auto/semi automatic annotation tool so I chose a CVAT. I added a feature to auto annotate because the models CVAT provides didn't have enough precision. How to create new auto annotation feature in CVAT \u00b6 The steps to add semi auto annotation feature is like this. Create serverless function description file. Create handler that process the request from the CVAT and returns response Create model handler that call model. There's an example branch I made to add an latest interactive segmentation model from sumsung. You can deploy this function to the nuclio by switching to my branch and running the command below. nuctl deploy --project-name cvat --path pytorch/ritm_interactive_segmentation --file pytorch/ritm_interactive_segmentation/function-gpu.yaml I guess you can create automatic labeling feature by adding similar files. Data collection \u00b6 It depends on application, but I collected data from cooking videos, because that contains scenes of hand holding things and can be a good training dataset. Annotation \u00b6 It takes around 2 hours to create 300 images this is around 10[s/image] in average. So not bad for a semantic segmentation annotation. Data will be published later. Currently I cannot make it public because I couldn't find suitable storage service for this. Conclusion \u00b6 interactive segmentation model is really useful and already practical model. Cvat is easily extensible and I recommend it.","title":"How I annotated 300 images with instance segmentation mask"},{"location":"dataset/02_instance_segmentation_annotation/#how-i-annotated-300-images-with-instance-segmentation-mask","text":"This is an article about how I annotate hand dataset fast. Firstly I'll explain annotation data formats and what I chose because this is important to choose the annotation tol.","title":"How I annotated 300 images with instance segmentation mask"},{"location":"dataset/02_instance_segmentation_annotation/#annotation-data-formats","text":"There are several segmentation tasks and which format should we choose? Panoptic segmentation format of the coco dataset seems flexible because the panoptic dataset can be used to train instance segmentation, semantic segmentation, panoptic segmentation. Briefly, the data format of the panoptic annotation is png image and json file. Each pixel of the image contains segment ids and the json file contains class_id and other metadata of the segments. The detail of the data format is there . Unfortunately there's no annotation tools that supports the panoptic annotation format. So we need other options. One of the other options is VOC format . That is supported by many tools. The png image has a class id and there's no distinction between different instances. Other option is instance segmentation format. Coco format can contain segment information for each objects and we can utilize it. I think the best format depends on task to solve. For some case, if the target scene to segment only contains stuff, we can choose semantic segmentation and the target scenes to segment only contain things we can choose instance segmentation format. You can find the informal explanation of things/stuff here .","title":"Annotation data formats"},{"location":"dataset/02_instance_segmentation_annotation/#tools","text":"There are some tools that can be used to segmentation tasks. Unfortunately AFAIK, there's no tools that can output panoptic coco format. So currently we need to choose an annotation tool that supports both coco instance segmentation format and voc segmentation format. Labelme and CVAT both supports both formats. There's a brief description of these tools.","title":"Tools"},{"location":"dataset/02_instance_segmentation_annotation/#labelme","text":"Labelme is really simple tool that support folder based data/task management and supports coco like format for instance segmentation and VOC like for semantic segmentation. This is all written in python and it can be easily modified by software engineer in machine learning.","title":"Labelme"},{"location":"dataset/02_instance_segmentation_annotation/#cvat","text":"CVAT is annotation + data/annotation task management platform. This is really nice for larger scale team. This tool is It is written in Typescript and need someone who knows Typescript. It has a auto/semi automatic annotation tools that can be potentially helpful and it can be extensible. About dataset, it can export both VOC like format and COCO format.","title":"CVAT"},{"location":"dataset/02_instance_segmentation_annotation/#example-hand-annotation-task","text":"For a hand annotation task, exporting a file as coco format is suitable, because human body are things (not stuff) and it is better to treat them as a instance segmentation. This is natural because when imagining image editing application, we want to distinguish And for my case, I wanted to use auto/semi automatic annotation tool so I chose a CVAT. I added a feature to auto annotate because the models CVAT provides didn't have enough precision.","title":"Example: Hand Annotation Task"},{"location":"dataset/02_instance_segmentation_annotation/#how-to-create-new-auto-annotation-feature-in-cvat","text":"The steps to add semi auto annotation feature is like this. Create serverless function description file. Create handler that process the request from the CVAT and returns response Create model handler that call model. There's an example branch I made to add an latest interactive segmentation model from sumsung. You can deploy this function to the nuclio by switching to my branch and running the command below. nuctl deploy --project-name cvat --path pytorch/ritm_interactive_segmentation --file pytorch/ritm_interactive_segmentation/function-gpu.yaml I guess you can create automatic labeling feature by adding similar files.","title":"How to create new auto annotation feature in CVAT"},{"location":"dataset/02_instance_segmentation_annotation/#data-collection","text":"It depends on application, but I collected data from cooking videos, because that contains scenes of hand holding things and can be a good training dataset.","title":"Data collection"},{"location":"dataset/02_instance_segmentation_annotation/#annotation","text":"It takes around 2 hours to create 300 images this is around 10[s/image] in average. So not bad for a semantic segmentation annotation. Data will be published later. Currently I cannot make it public because I couldn't find suitable storage service for this.","title":"Annotation"},{"location":"dataset/02_instance_segmentation_annotation/#conclusion","text":"interactive segmentation model is really useful and already practical model. Cvat is easily extensible and I recommend it.","title":"Conclusion"},{"location":"dataset/03_dataset_construction_by_reconstruction/","text":"Reconstruction based dataset construction \u00b6","title":"Reconstruction based dataset construction"},{"location":"dataset/03_dataset_construction_by_reconstruction/#reconstruction-based-dataset-construction","text":"","title":"Reconstruction based dataset construction"}]}