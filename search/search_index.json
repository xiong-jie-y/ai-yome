{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This website is mostly articles about computer vision technologies.","title":"Home"},{"location":"my_profile/","text":"Profile \u00b6 I'm a software engineer and sometimes machine learning engineer. Some of my projects can be the basis for ai wife achievement. Twitter account is here . I need support for my projects to create my software (mostly cloud GPU fee and real GPU fee) and test new sensors and actuators. Please help me :D on my github sponsor account .","title":"Profile"},{"location":"my_profile/#profile","text":"I'm a software engineer and sometimes machine learning engineer. Some of my projects can be the basis for ai wife achievement. Twitter account is here . I need support for my projects to create my software (mostly cloud GPU fee and real GPU fee) and test new sensors and actuators. Please help me :D on my github sponsor account .","title":"Profile"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/","text":"The brief survey of the 3D video reconstruction for Looking Glass \u00b6 In this article, I will explain overview of the methods to create 3D Videos for Looking Glass. This article is for the people who want to create apps to create 3D Videos and not for app users. The target video is any kind of video, for a video where multiple people is moving as a foreground and foreground is fixed, the method I implemented in remimi can be used. I will explain the method in another article. Firstly I will explain video format we want to output, and secondly the methods that is useful to output the video. 3D Video Format we adopt \u00b6 Before explaining video format, I explain apps available. There are some apps for Looking Glass. To play 3D Videos on Looking Glass, you can use HoloPlay Studio. The supported video formats are Quilt Video RGBD Video The format is really simple, quilt video contains images rfom all the views per frame and RGBD video contains a rgb image and a depth image per frame. The software only supports windows and mac. For ubuntu, you need to write your quilt video player. I recommend using Unity Plugin because it mostly requires just clicking and few types. To record these videos, there's a DepthRecorder . This only supports azure kinect on Windows, but you can record RGBD video using this software. The quilt video format allow us to put more algorithm, meaning there's more room to improvement. So I chose quilt video generation for it. The research survey for quilt video generation \u00b6 Quilt video contains quilt image per frame. Quilt image is a collection of images from views of horizontally different angles. Thus to generate this quilt image, we need to prepare images from horizontally different views. I surveyed researches for the sensor configurations that can be constructed within $1000 USD cost. End-to-End View Synthesis from single or two rgb image \u00b6 There are some researches that synthesis novel view images from a single view or two views (stereo camera) in a end to end manner. Google's models output a MPI image and from the MPI image novel view image is synthesized. The MPI image is lightweight expression of a scene of thin layer and can be used to render novel view images fast. There are methods for stereo images and a single image . The two images below are the network structure for stereo images and a single image each. These networks output MPI images and novel view images are rendered from MPI images. Facebook's Synsin approach predict a rgb image given a rgb image and a relative pose to the input rgb image. The model is depicted in the image below. All of the model is trained on limited dataset (few number of indoor and outdoor dataset), but the facebook's Synsin model is more generalizable than other models from the evaluation result of training the model on outdoor dataset and test on indoor dataset. These are the images from the facebook's animation. Dis-occluded region is dirty and apparent. Look at the original GIF for check. These hallucinated regions are a bit dirtier than the single inpainting methods, but it's not a big difference when applying to video, because they both don't have a consistency between video frames. I guess that this model learn's something like how to inpaint the dis-occluded region from the viewpoint change. The demo of the facebook model seems not perfect, but it might be used to video on narrower HFOV 3D monitors like Looking Glass. Also this is a good starting point for the 3D video generator for any videos. Hand crafted view synthesis from single RGB or RGBD image \u00b6 Hand crafted way is firstly convert into point cloud and render point cloud from different views for quilt image generation. It is easy to create quilt image from RGBD because there's already depth channel and point cloud can be reconstructed. We can just generate quilt video from the point cloud by moving virtual camera horizontally. For an RGB image, the Depth channel can be estimated from monocular depth estimation methods like DPT. For this approach, one of the two primary problem we want to solve is inpainting of dis-occluded region and boundary problem. Inpainting of dis-occluded region \u00b6 There are two ways of inpainting, inpainting depth channel and rgb channels separately or jointly. In \"3D Ken Burns Effect from a Single Image\" , they constructed two networks. One is the network to extract context region used for inpaint, and the other network is inpainting network that output inpainted RGBD image given RGBD image and context vector. In \"3D Photography using Context-aware Layered Depth Inpainting\" , the inpainting process has two stages as you can see in the figure below.Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. The reason might be that the inpainting model utilize edge in the second method and it preserves edge better. Firstly edge image is inpainted and inpainted edge is given to to the second network along with RGB channel and depth channel. Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. Either of these inpainting method is the method for single image and applying to video might cause flickering of depth and color channel. It's ok for a single image 3d reconstruction, but it might not be enough for 3d video generation. Boundary of the objects \u00b6 Another problem is boundary, with most of the depth estimation method, boundary is not clear. Also boundary is very sparse and there's continuity between background and objects. These point looks garbages when seeing 3d monitors. One of the solution is simple filters, but it doesn't fully solve the problem and another solution will be the reconstruction based on the knowledge of objects. For example, there's many way to reconstruct an human 3d mesh from single RGB image using the knowledge of the human shape. For example, PiFU learns human shape and reconstruct an high resolution human mesh from a single RGB image. SMPL is a parameterized body shape constructed from data, and often used by the model to extract body mesh shape from a single RGB image. Currently it is difficult to model the clothed people, and one of the successful method is SCANimate . It seems that there's still need to be advances for handling boundary of the objects. Conclusion \u00b6 Currently Synsin seems the balanced and well performing method to synthesize novel views from wide range of images and videos. For a better quality, I might take either of ways. Increase data for synsin and train them more. (more than 3-6 days for one training.) Try handcrafted way using multiple models. Either of it seems difficult, but I will try after implementing Synsin.","title":"The brief survey of the 3D video reconstruction for Looking Glass"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#the-brief-survey-of-the-3d-video-reconstruction-for-looking-glass","text":"In this article, I will explain overview of the methods to create 3D Videos for Looking Glass. This article is for the people who want to create apps to create 3D Videos and not for app users. The target video is any kind of video, for a video where multiple people is moving as a foreground and foreground is fixed, the method I implemented in remimi can be used. I will explain the method in another article. Firstly I will explain video format we want to output, and secondly the methods that is useful to output the video.","title":"The brief survey of the 3D video reconstruction for Looking Glass"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#3d-video-format-we-adopt","text":"Before explaining video format, I explain apps available. There are some apps for Looking Glass. To play 3D Videos on Looking Glass, you can use HoloPlay Studio. The supported video formats are Quilt Video RGBD Video The format is really simple, quilt video contains images rfom all the views per frame and RGBD video contains a rgb image and a depth image per frame. The software only supports windows and mac. For ubuntu, you need to write your quilt video player. I recommend using Unity Plugin because it mostly requires just clicking and few types. To record these videos, there's a DepthRecorder . This only supports azure kinect on Windows, but you can record RGBD video using this software. The quilt video format allow us to put more algorithm, meaning there's more room to improvement. So I chose quilt video generation for it.","title":"3D Video Format we adopt"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#the-research-survey-for-quilt-video-generation","text":"Quilt video contains quilt image per frame. Quilt image is a collection of images from views of horizontally different angles. Thus to generate this quilt image, we need to prepare images from horizontally different views. I surveyed researches for the sensor configurations that can be constructed within $1000 USD cost.","title":"The research survey for quilt video generation"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#end-to-end-view-synthesis-from-single-or-two-rgb-image","text":"There are some researches that synthesis novel view images from a single view or two views (stereo camera) in a end to end manner. Google's models output a MPI image and from the MPI image novel view image is synthesized. The MPI image is lightweight expression of a scene of thin layer and can be used to render novel view images fast. There are methods for stereo images and a single image . The two images below are the network structure for stereo images and a single image each. These networks output MPI images and novel view images are rendered from MPI images. Facebook's Synsin approach predict a rgb image given a rgb image and a relative pose to the input rgb image. The model is depicted in the image below. All of the model is trained on limited dataset (few number of indoor and outdoor dataset), but the facebook's Synsin model is more generalizable than other models from the evaluation result of training the model on outdoor dataset and test on indoor dataset. These are the images from the facebook's animation. Dis-occluded region is dirty and apparent. Look at the original GIF for check. These hallucinated regions are a bit dirtier than the single inpainting methods, but it's not a big difference when applying to video, because they both don't have a consistency between video frames. I guess that this model learn's something like how to inpaint the dis-occluded region from the viewpoint change. The demo of the facebook model seems not perfect, but it might be used to video on narrower HFOV 3D monitors like Looking Glass. Also this is a good starting point for the 3D video generator for any videos.","title":"End-to-End View Synthesis from single or two rgb image"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#hand-crafted-view-synthesis-from-single-rgb-or-rgbd-image","text":"Hand crafted way is firstly convert into point cloud and render point cloud from different views for quilt image generation. It is easy to create quilt image from RGBD because there's already depth channel and point cloud can be reconstructed. We can just generate quilt video from the point cloud by moving virtual camera horizontally. For an RGB image, the Depth channel can be estimated from monocular depth estimation methods like DPT. For this approach, one of the two primary problem we want to solve is inpainting of dis-occluded region and boundary problem.","title":"Hand crafted view synthesis from single RGB or RGBD image"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#inpainting-of-dis-occluded-region","text":"There are two ways of inpainting, inpainting depth channel and rgb channels separately or jointly. In \"3D Ken Burns Effect from a Single Image\" , they constructed two networks. One is the network to extract context region used for inpaint, and the other network is inpainting network that output inpainted RGBD image given RGBD image and context vector. In \"3D Photography using Context-aware Layered Depth Inpainting\" , the inpainting process has two stages as you can see in the figure below.Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. The reason might be that the inpainting model utilize edge in the second method and it preserves edge better. Firstly edge image is inpainted and inpainted edge is given to to the second network along with RGB channel and depth channel. Two RGBD inpainting method is not thoroughly evaluated, but from the inpainted images, second one seems better. Either of these inpainting method is the method for single image and applying to video might cause flickering of depth and color channel. It's ok for a single image 3d reconstruction, but it might not be enough for 3d video generation.","title":"Inpainting of dis-occluded region"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#boundary-of-the-objects","text":"Another problem is boundary, with most of the depth estimation method, boundary is not clear. Also boundary is very sparse and there's continuity between background and objects. These point looks garbages when seeing 3d monitors. One of the solution is simple filters, but it doesn't fully solve the problem and another solution will be the reconstruction based on the knowledge of objects. For example, there's many way to reconstruct an human 3d mesh from single RGB image using the knowledge of the human shape. For example, PiFU learns human shape and reconstruct an high resolution human mesh from a single RGB image. SMPL is a parameterized body shape constructed from data, and often used by the model to extract body mesh shape from a single RGB image. Currently it is difficult to model the clothed people, and one of the successful method is SCANimate . It seems that there's still need to be advances for handling boundary of the objects.","title":"Boundary of the objects"},{"location":"3d_reconstruction/01_survey_3d_reconstruction/#conclusion","text":"Currently Synsin seems the balanced and well performing method to synthesize novel views from wide range of images and videos. For a better quality, I might take either of ways. Increase data for synsin and train them more. (more than 3-6 days for one training.) Try handcrafted way using multiple models. Either of it seems difficult, but I will try after implementing Synsin.","title":"Conclusion"},{"location":"3d_reconstruction/02_try_syinsin/","text":"\u00b6 Takeaway \u00b6 The synsin can be a nice baseline for the novel view synthesis task if it's not for realtime novel view synthesis.","title":"02 try syinsin"},{"location":"3d_reconstruction/02_try_syinsin/#_1","text":"","title":""},{"location":"3d_reconstruction/02_try_syinsin/#takeaway","text":"The synsin can be a nice baseline for the novel view synthesis task if it's not for realtime novel view synthesis.","title":"Takeaway"},{"location":"dataset/01_why_annotation/","text":"why dataset construction article? \u00b6 The dataset construction is important to train a good ML model. Both the quality and the quantity is really important for the dataset. The mediapipe's hand keypoint detection model is trained with more than 100000 synthetic CG images and real images. But it's still not perfect and if you need more performance you need to collect more data with high quality annotation. Also the image below shows that the better quality data result in a better performance model. (from this slid e). Achieving both quality and quantity is sometimes difficult for some tasks. To increase the number of the annotated data, it will take time linearly increased to the number of annotated data. If the number of the data is increased, the quality control of the annotation will take more time. So both achieving quality and the quantity costs a lot. Thus if your responsibility is getting a good model, it is worth taking time to learn how to construct an dataset efficiently and with a good quality for various tasks. For this purpose I wrote an article about dataset construction for each task from research and industries. In my articles, I will try to focus on methods that meet one of these conditions to deliver a nice quality information to the readers. the ones that can enable something that can be done without the method the ones that can let us construct dataset 2-100 times more efficient the ones that can be used widespread in many tasks (e.g. not a UI improvement for a specific tasks)","title":"why dataset construction article?"},{"location":"dataset/01_why_annotation/#why-dataset-construction-article","text":"The dataset construction is important to train a good ML model. Both the quality and the quantity is really important for the dataset. The mediapipe's hand keypoint detection model is trained with more than 100000 synthetic CG images and real images. But it's still not perfect and if you need more performance you need to collect more data with high quality annotation. Also the image below shows that the better quality data result in a better performance model. (from this slid e). Achieving both quality and quantity is sometimes difficult for some tasks. To increase the number of the annotated data, it will take time linearly increased to the number of annotated data. If the number of the data is increased, the quality control of the annotation will take more time. So both achieving quality and the quantity costs a lot. Thus if your responsibility is getting a good model, it is worth taking time to learn how to construct an dataset efficiently and with a good quality for various tasks. For this purpose I wrote an article about dataset construction for each task from research and industries. In my articles, I will try to focus on methods that meet one of these conditions to deliver a nice quality information to the readers. the ones that can enable something that can be done without the method the ones that can let us construct dataset 2-100 times more efficient the ones that can be used widespread in many tasks (e.g. not a UI improvement for a specific tasks)","title":"why dataset construction article?"},{"location":"dataset/02_instance_segmentation_annotation/","text":"How I annotated 300 images with instance segmentation mask \u00b6 This is an article about how I annotate hand dataset fast. Firstly I'll explain annotation data formats and what I chose because this is important to choose the annotation tol. Annotation data formats \u00b6 There are several segmentation tasks and which format should we choose? Panoptic segmentation format of the coco dataset seems flexible because the panoptic dataset can be used to train instance segmentation, semantic segmentation, panoptic segmentation. Briefly, the data format of the panoptic annotation is png image and json file. Each pixel of the image contains segment ids and the json file contains class_id and other metadata of the segments. The detail of the data format is there . Unfortunately there's no annotation tools that supports the panoptic annotation format. So we need other options. One of the other options is VOC format . That is supported by many tools. The png image has a class id and there's no distinction between different instances. Other option is instance segmentation format. Coco format can contain segment information for each objects and we can utilize it. I think the best format depends on task to solve. For some case, if the target scene to segment only contains stuff, we can choose semantic segmentation and the target scenes to segment only contain things we can choose instance segmentation format. You can find the informal explanation of things/stuff here . Tools \u00b6 There are some tools that can be used to segmentation tasks. Unfortunately AFAIK, there's no tools that can output panoptic coco format. So currently we need to choose an annotation tool that supports both coco instance segmentation format and voc segmentation format. Labelme and CVAT both supports both formats. There's a brief description of these tools. Labelme \u00b6 Labelme is really simple tool that support folder based data/task management and supports coco like format for instance segmentation and VOC like for semantic segmentation. This is all written in python and it can be easily modified by software engineer in machine learning. CVAT \u00b6 CVAT is annotation + data/annotation task management platform. This is really nice for larger scale team. This tool is It is written in Typescript and need someone who knows Typescript. It has a auto/semi automatic annotation tools that can be potentially helpful and it can be extensible. About dataset, it can export both VOC like format and COCO format. Example: Hand Annotation Task \u00b6 For a hand annotation task, exporting a file as coco format is suitable, because human body are things (not stuff) and it is better to treat them as a instance segmentation. This is natural because when imagining image editing application, we want to distinguish And for my case, I wanted to use auto/semi automatic annotation tool so I chose a CVAT. I added a feature to auto annotate because the models CVAT provides didn't have enough precision. How to create new auto annotation feature in CVAT \u00b6 The steps to add semi auto annotation feature is like this. Create serverless function description file. Create handler that process the request from the CVAT and returns response Create model handler that call model. There's an example branch I made to add an latest interactive segmentation model from sumsung. You can deploy this function to the nuclio by switching to my branch and running the command below. nuctl deploy --project-name cvat --path pytorch/ritm_interactive_segmentation --file pytorch/ritm_interactive_segmentation/function-gpu.yaml I guess you can create automatic labeling feature by adding similar files. Data collection \u00b6 It depends on application, but I collected data from cooking videos, because that contains scenes of hand holding things and can be a good training dataset. Annotation \u00b6 It takes around 2 hours to create 300 images this is around 10[s/image] in average. So not bad for a semantic segmentation annotation. Data will be published later. Currently I cannot make it public because I couldn't find suitable storage service for this. Conclusion \u00b6 interactive segmentation model is really useful and already practical model. Cvat is easily extensible and I recommend it.","title":"How I annotated 300 images with instance segmentation mask"},{"location":"dataset/02_instance_segmentation_annotation/#how-i-annotated-300-images-with-instance-segmentation-mask","text":"This is an article about how I annotate hand dataset fast. Firstly I'll explain annotation data formats and what I chose because this is important to choose the annotation tol.","title":"How I annotated 300 images with instance segmentation mask"},{"location":"dataset/02_instance_segmentation_annotation/#annotation-data-formats","text":"There are several segmentation tasks and which format should we choose? Panoptic segmentation format of the coco dataset seems flexible because the panoptic dataset can be used to train instance segmentation, semantic segmentation, panoptic segmentation. Briefly, the data format of the panoptic annotation is png image and json file. Each pixel of the image contains segment ids and the json file contains class_id and other metadata of the segments. The detail of the data format is there . Unfortunately there's no annotation tools that supports the panoptic annotation format. So we need other options. One of the other options is VOC format . That is supported by many tools. The png image has a class id and there's no distinction between different instances. Other option is instance segmentation format. Coco format can contain segment information for each objects and we can utilize it. I think the best format depends on task to solve. For some case, if the target scene to segment only contains stuff, we can choose semantic segmentation and the target scenes to segment only contain things we can choose instance segmentation format. You can find the informal explanation of things/stuff here .","title":"Annotation data formats"},{"location":"dataset/02_instance_segmentation_annotation/#tools","text":"There are some tools that can be used to segmentation tasks. Unfortunately AFAIK, there's no tools that can output panoptic coco format. So currently we need to choose an annotation tool that supports both coco instance segmentation format and voc segmentation format. Labelme and CVAT both supports both formats. There's a brief description of these tools.","title":"Tools"},{"location":"dataset/02_instance_segmentation_annotation/#labelme","text":"Labelme is really simple tool that support folder based data/task management and supports coco like format for instance segmentation and VOC like for semantic segmentation. This is all written in python and it can be easily modified by software engineer in machine learning.","title":"Labelme"},{"location":"dataset/02_instance_segmentation_annotation/#cvat","text":"CVAT is annotation + data/annotation task management platform. This is really nice for larger scale team. This tool is It is written in Typescript and need someone who knows Typescript. It has a auto/semi automatic annotation tools that can be potentially helpful and it can be extensible. About dataset, it can export both VOC like format and COCO format.","title":"CVAT"},{"location":"dataset/02_instance_segmentation_annotation/#example-hand-annotation-task","text":"For a hand annotation task, exporting a file as coco format is suitable, because human body are things (not stuff) and it is better to treat them as a instance segmentation. This is natural because when imagining image editing application, we want to distinguish And for my case, I wanted to use auto/semi automatic annotation tool so I chose a CVAT. I added a feature to auto annotate because the models CVAT provides didn't have enough precision.","title":"Example: Hand Annotation Task"},{"location":"dataset/02_instance_segmentation_annotation/#how-to-create-new-auto-annotation-feature-in-cvat","text":"The steps to add semi auto annotation feature is like this. Create serverless function description file. Create handler that process the request from the CVAT and returns response Create model handler that call model. There's an example branch I made to add an latest interactive segmentation model from sumsung. You can deploy this function to the nuclio by switching to my branch and running the command below. nuctl deploy --project-name cvat --path pytorch/ritm_interactive_segmentation --file pytorch/ritm_interactive_segmentation/function-gpu.yaml I guess you can create automatic labeling feature by adding similar files.","title":"How to create new auto annotation feature in CVAT"},{"location":"dataset/02_instance_segmentation_annotation/#data-collection","text":"It depends on application, but I collected data from cooking videos, because that contains scenes of hand holding things and can be a good training dataset.","title":"Data collection"},{"location":"dataset/02_instance_segmentation_annotation/#annotation","text":"It takes around 2 hours to create 300 images this is around 10[s/image] in average. So not bad for a semantic segmentation annotation. Data will be published later. Currently I cannot make it public because I couldn't find suitable storage service for this.","title":"Annotation"},{"location":"dataset/02_instance_segmentation_annotation/#conclusion","text":"interactive segmentation model is really useful and already practical model. Cvat is easily extensible and I recommend it.","title":"Conclusion"},{"location":"dataset/03_model_improvement_by_active_learning/","text":"Training better hand instance segmentation model \u00b6 In the previous article, I constructed 300 hand segmentation model with interactive segmentation feature. I merged the dataset with the hand segmentation dataset constructed with ozeu. The ozeu provide and scripts to create coco dataset from the output one and merge them. The running result of the trained mode ls below. This is improved than previously. But it still has failure cases in multiple hand In this article, I will improve these weaknesses by active learning and autolabeling process. The active learning dataset generation with cvat is supported in ozeu. The steps to do active learning is Collect data Run model on it and get the lower confident result and higher confident results. Fix wrong auto annotated results and lower confident results with cvat. Retrain the model with newly annotated data and repeat the process from 2. So the actual procedure using actual tools will follow. Collect data \u00b6 For the hand dataset Get the labeling target \u00b6 Fix it \u00b6 Retrain the model \u00b6 Result \u00b6 This is the result of retraining the model. The failure cases of are fixed. But ... still needs to be improved. Conclusions \u00b6 The active learning can be used to improve weaknesses of the model. In the next article, I will work on the improvement of the model with synthetic data generation.","title":"Training better hand instance segmentation model"},{"location":"dataset/03_model_improvement_by_active_learning/#training-better-hand-instance-segmentation-model","text":"In the previous article, I constructed 300 hand segmentation model with interactive segmentation feature. I merged the dataset with the hand segmentation dataset constructed with ozeu. The ozeu provide and scripts to create coco dataset from the output one and merge them. The running result of the trained mode ls below. This is improved than previously. But it still has failure cases in multiple hand In this article, I will improve these weaknesses by active learning and autolabeling process. The active learning dataset generation with cvat is supported in ozeu. The steps to do active learning is Collect data Run model on it and get the lower confident result and higher confident results. Fix wrong auto annotated results and lower confident results with cvat. Retrain the model with newly annotated data and repeat the process from 2. So the actual procedure using actual tools will follow.","title":"Training better hand instance segmentation model"},{"location":"dataset/03_model_improvement_by_active_learning/#collect-data","text":"For the hand dataset","title":"Collect data"},{"location":"dataset/03_model_improvement_by_active_learning/#get-the-labeling-target","text":"","title":"Get the labeling target"},{"location":"dataset/03_model_improvement_by_active_learning/#fix-it","text":"","title":"Fix it"},{"location":"dataset/03_model_improvement_by_active_learning/#retrain-the-model","text":"","title":"Retrain the model"},{"location":"dataset/03_model_improvement_by_active_learning/#result","text":"This is the result of retraining the model. The failure cases of are fixed. But ... still needs to be improved.","title":"Result"},{"location":"dataset/03_model_improvement_by_active_learning/#conclusions","text":"The active learning can be used to improve weaknesses of the model. In the next article, I will work on the improvement of the model with synthetic data generation.","title":"Conclusions"},{"location":"dataset/04_model_improvement_by_data_augmentation/","text":"","title":"04 model improvement by data augmentation"},{"location":"erotech/01_erotic_computer_vision/","text":"Erotic computer vision \u00b6 Erotic Applications \u00b6 I will train some computer vision models for erotic applications. In this article, the models that will be developed in near future will be explained. The applications are App1: The robotic arm masturbator moves synchronized with video App2: The video or sound controller by motion detection of hand or masturbator App 1 has big impact to many applications. For example, the robot arm motion can not only synchronized with ordinary video but also with VR video or VR app. So it has really big potential. App 2 has a narrower impact than the App 1 because the video or sound will be controlled by the motion, so there's some delay between the motion and the play of the video or the sound. The target objects to detect are hand: App2 the genital of the man: App1, App2 masturbator: App1, App2 In the remaining of this article will be how to detect and create models. Decide how to detect objects \u00b6 The methods to detect these objects can be 2d bounding box detection in image plane instance segmentation in image plane 3d object detection 6DoF estimation shape and pose estimation 2d keypoint detection 3d keypoint detection We need to choose the most suitable methods for each objects considering applications. We need to consider applications because it will decide requirements of the performance and each task has a different difficulties of collecting dataset and training the model. It can be rephrased by decide the goal and make the requirements of the model first. In this case, some application will be developed later experimentally. So the collection of the application defines requirement of the model. Additional requirement is sensor is RGB camera or RGBD camera and model should run realtime. (30-60Hz) Hand recognition \u00b6 The dataset collection is easy, but getting the diversity is a bit hard. Mostly synthetic data and the internet images are the source of dataset. Application only requires the algorithm to detect the simple motion and the shape of the hand. So 3d or 2d keypoint detection might be enough. Genital recognition \u00b6 Probably it's possible to get images of them from the internet but recording the new data with richer ground truth from other sensors is a bit difficult. For example, 3d keypoint detection dataset is often constructed from the multiple cameras and depth sensors. But recording new data with diversity is a bit difficult. Also can rely on synthetic data, but we should also get real data. The requirement from the application is that it should detect shape of the object in detail, because the program will control their arm to touch the genital effectively. It should know recognized part is which part of the genital. The algorithm also need to handle different size and the shape. The options given this limitation will be the deep learning models which input is RGB and the ground truth label can be annotated given image or videos. The suitable one is instance segmentation because it can be combined with depth information and can be used as a shape estimation. So it is required to solve the problem, recognized which part is which part by unsupervised shape and pose estimation or the handcrafted logic and instance segmentation or other methods. Masturbator recognition \u00b6 The dataset construction is possible It can be constructed from the images in the internet and also I can acquire new dataset by buying them and taking photos of them. The requirement of the application is knowing the direction of the masturbator. For some cases, it doesn't need to handle shape change because shape change is not so large problem, but for some case, it need to handle shape deformation. So given this, the 6DoF or the shape and pose estimation is the direction to explorer, balancing the difficulty and the effect. Conclusion \u00b6 I decided the direction to explorer based on the application requirements and the ML task candidates. The task will be Hand: 3D Keypoint detection Genital: Shape and pose estimation, instance segmentation Masturbator: Shape and pose estimation, 6DoF estimation This is the overall direction. In the next chapter, I will survey each area and decide what to create.","title":"Erotic computer vision"},{"location":"erotech/01_erotic_computer_vision/#erotic-computer-vision","text":"","title":"Erotic computer vision"},{"location":"erotech/01_erotic_computer_vision/#erotic-applications","text":"I will train some computer vision models for erotic applications. In this article, the models that will be developed in near future will be explained. The applications are App1: The robotic arm masturbator moves synchronized with video App2: The video or sound controller by motion detection of hand or masturbator App 1 has big impact to many applications. For example, the robot arm motion can not only synchronized with ordinary video but also with VR video or VR app. So it has really big potential. App 2 has a narrower impact than the App 1 because the video or sound will be controlled by the motion, so there's some delay between the motion and the play of the video or the sound. The target objects to detect are hand: App2 the genital of the man: App1, App2 masturbator: App1, App2 In the remaining of this article will be how to detect and create models.","title":"Erotic Applications"},{"location":"erotech/01_erotic_computer_vision/#decide-how-to-detect-objects","text":"The methods to detect these objects can be 2d bounding box detection in image plane instance segmentation in image plane 3d object detection 6DoF estimation shape and pose estimation 2d keypoint detection 3d keypoint detection We need to choose the most suitable methods for each objects considering applications. We need to consider applications because it will decide requirements of the performance and each task has a different difficulties of collecting dataset and training the model. It can be rephrased by decide the goal and make the requirements of the model first. In this case, some application will be developed later experimentally. So the collection of the application defines requirement of the model. Additional requirement is sensor is RGB camera or RGBD camera and model should run realtime. (30-60Hz)","title":"Decide how to detect objects"},{"location":"erotech/01_erotic_computer_vision/#hand-recognition","text":"The dataset collection is easy, but getting the diversity is a bit hard. Mostly synthetic data and the internet images are the source of dataset. Application only requires the algorithm to detect the simple motion and the shape of the hand. So 3d or 2d keypoint detection might be enough.","title":"Hand recognition"},{"location":"erotech/01_erotic_computer_vision/#genital-recognition","text":"Probably it's possible to get images of them from the internet but recording the new data with richer ground truth from other sensors is a bit difficult. For example, 3d keypoint detection dataset is often constructed from the multiple cameras and depth sensors. But recording new data with diversity is a bit difficult. Also can rely on synthetic data, but we should also get real data. The requirement from the application is that it should detect shape of the object in detail, because the program will control their arm to touch the genital effectively. It should know recognized part is which part of the genital. The algorithm also need to handle different size and the shape. The options given this limitation will be the deep learning models which input is RGB and the ground truth label can be annotated given image or videos. The suitable one is instance segmentation because it can be combined with depth information and can be used as a shape estimation. So it is required to solve the problem, recognized which part is which part by unsupervised shape and pose estimation or the handcrafted logic and instance segmentation or other methods.","title":"Genital recognition"},{"location":"erotech/01_erotic_computer_vision/#masturbator-recognition","text":"The dataset construction is possible It can be constructed from the images in the internet and also I can acquire new dataset by buying them and taking photos of them. The requirement of the application is knowing the direction of the masturbator. For some cases, it doesn't need to handle shape change because shape change is not so large problem, but for some case, it need to handle shape deformation. So given this, the 6DoF or the shape and pose estimation is the direction to explorer, balancing the difficulty and the effect.","title":"Masturbator recognition"},{"location":"erotech/01_erotic_computer_vision/#conclusion","text":"I decided the direction to explorer based on the application requirements and the ML task candidates. The task will be Hand: 3D Keypoint detection Genital: Shape and pose estimation, instance segmentation Masturbator: Shape and pose estimation, 6DoF estimation This is the overall direction. In the next chapter, I will survey each area and decide what to create.","title":"Conclusion"},{"location":"erotech/02_shape_and_pose_estimation/","text":"Shape and Pose estimation for genital and masturbator \u00b6 Shape and pose estimation is the methods to estimate shape and the pose at the same time.","title":"Shape and Pose estimation for genital and masturbator"},{"location":"erotech/02_shape_and_pose_estimation/#shape-and-pose-estimation-for-genital-and-masturbator","text":"Shape and pose estimation is the methods to estimate shape and the pose at the same time.","title":"Shape and Pose estimation for genital and masturbator"},{"location":"erotech/03_dataset_construction/","text":"Data Construction Techniques \u00b6 LabelFusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes \u00b6 Label fusion is the method to semi automatically annotate 6DoF pose and segmentation of objects from RGBD video of the static scene. The method first reconstruct the scene using the SLAM techniques from the RGBD videos and let users to select 3 points from the point of the object in th reconstructed scene and from these three point the mesh is place on the scene and the 6DoF pose of the object can be calculated (also the 6DoF pose is refined by ICP). It can annotate 6DoF of an object by clicking 3 times per an object per scene and thus enable getting vast amount of 6DoF data. Also not only getting vast amount of data, it can annotate cluttered scene that can contribute to the performance of semantic segmentation performance in the experiment. I think it can also be used as a 6DoF pose estimation dataset. But not sure it's implemented in the software or not. HOnnotate: A method for 3D Annotation of Hand and Object Poses \u00b6 This is the automatic annotation methods for the 6DoF pose of objects and the 3d keypoints of hands. Given RGBD videos from single or multiple videos, the method can produce annotations. The things that's necessary for this methods are The precise instance or semantic segmentation model of a hand and the object 3D models of hand and the object The precise 2d keypoint detection model of a hand There's already many precise 2d keypoint detection model of the hand (e.g. mediapipe hand landmark detector). Extra work is depending on the task which dataset you want to get. As a 6DoF data annotation method, so compared to the other method, extra work is creating a instance segmentation. If we combine this method with a LabelFusion it might be a really nice 6DoF dataset generation process because LabelFusion can generate both 6DoF and the instance segmentation dataset. As a 3d keypoint detection dataset construction method, objects can be any, so it is highly possible that we can just reuse the existing dataset or the objects. Conclusion \u00b6 The dataset construction methods for 6DoF pose relies on RGBD sequence and mesh. I think they are necessary because depth channel measured in meter will give 6DoF pose estimation method real measure an mesh will defined the unit direction of an objects. Especially getting the precise model is the really high hurdle to getting the data. I will explore the easy way to get the high precision model in another article.","title":"Data Construction Techniques"},{"location":"erotech/03_dataset_construction/#data-construction-techniques","text":"","title":"Data Construction Techniques"},{"location":"erotech/03_dataset_construction/#labelfusion-a-pipeline-for-generating-ground-truth-labels-for-real-rgbd-data-of-cluttered-scenes","text":"Label fusion is the method to semi automatically annotate 6DoF pose and segmentation of objects from RGBD video of the static scene. The method first reconstruct the scene using the SLAM techniques from the RGBD videos and let users to select 3 points from the point of the object in th reconstructed scene and from these three point the mesh is place on the scene and the 6DoF pose of the object can be calculated (also the 6DoF pose is refined by ICP). It can annotate 6DoF of an object by clicking 3 times per an object per scene and thus enable getting vast amount of 6DoF data. Also not only getting vast amount of data, it can annotate cluttered scene that can contribute to the performance of semantic segmentation performance in the experiment. I think it can also be used as a 6DoF pose estimation dataset. But not sure it's implemented in the software or not.","title":"LabelFusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes"},{"location":"erotech/03_dataset_construction/#honnotate-a-method-for-3d-annotation-of-hand-and-object-poses","text":"This is the automatic annotation methods for the 6DoF pose of objects and the 3d keypoints of hands. Given RGBD videos from single or multiple videos, the method can produce annotations. The things that's necessary for this methods are The precise instance or semantic segmentation model of a hand and the object 3D models of hand and the object The precise 2d keypoint detection model of a hand There's already many precise 2d keypoint detection model of the hand (e.g. mediapipe hand landmark detector). Extra work is depending on the task which dataset you want to get. As a 6DoF data annotation method, so compared to the other method, extra work is creating a instance segmentation. If we combine this method with a LabelFusion it might be a really nice 6DoF dataset generation process because LabelFusion can generate both 6DoF and the instance segmentation dataset. As a 3d keypoint detection dataset construction method, objects can be any, so it is highly possible that we can just reuse the existing dataset or the objects.","title":"HOnnotate: A method for 3D Annotation of Hand and Object Poses"},{"location":"erotech/03_dataset_construction/#conclusion","text":"The dataset construction methods for 6DoF pose relies on RGBD sequence and mesh. I think they are necessary because depth channel measured in meter will give 6DoF pose estimation method real measure an mesh will defined the unit direction of an objects. Especially getting the precise model is the really high hurdle to getting the data. I will explore the easy way to get the high precision model in another article.","title":"Conclusion"},{"location":"misc/01_contribution_to_cvat/","text":"How I added an interactive segmentation feature to CVAT. \u00b6 In this article, I will explain how it's easy to add a semi & auto annotation feature to CVAT. When the function is merged to CVAT, I felt joy because the function might be used by many users. So I really recommend contributing to OSS if you haven't. CVAT is a well developed annotation tool for computer vision tasks. One of the interesting features of CVAT is semi annotation & auto annotation feature. I wanted to use one of the interactive segmentation feature, which helps with annotating instance segmentation, but the features in the CVAT was not enough for my task. So I implemented a new interactive segmentation feature. The screenshot below is the screenshot of using the new feature. The polygon with black line and painted with white half transparent is the estimated segmentation with new semi annotation. The following is the note the way I merge the new feature to CVAT. What & How I implemented \u00b6 I already confirmed the basic functionality of CVAT and tried the annotation a bit. I tried an interactive segmentation following this procedure . I will recommend using them if you haven't used CVAT and want to contribute to them. The semi & automatic annotation is implemented as a server less function and what I need to add is just a serverless function and cvat will add the serverless function automatically to UI. By checking nuclio (serverless function management tool) document, and some config file, I guessed that what I need to write is mostly writing docker file for the container. So I read some config files and serverless function scripts under this directory and environment construction procedure of new interactive segmentation model and just wrote the new config files and serverless function script. The actual PR is here . Read contribution guideline \u00b6 There's a contribution guide for contributors. Usually the repository has a code style guideline for each language in the repository, but there seems not for yaml and python maybe because the code line is not so many. So I just see other code and followed their implicit code style. Clean code & Update documentation \u00b6 I cleaned code and yaml, just rename container name, serverless function name, conform the style to other code, update README, tutorial, and CHANGELOG. That's it. Respond to comments and fix it \u00b6 I didn't get a code fix request and I got a gratitude and code has merged. Takeaway \u00b6 This time merging PR was really easy & fun. Of course, difficulty of contributing OSS projects depends on project and functions. For example, frontend has a more strict guideline about automated testing and coding styles. So if are not front-end engineer, you should avoid contribute to this part (or maybe you should if you like to learn front-end part). So find issues or feature request from your favorite OSS, check the guideline and if you think you are confident you can contribute to the OSS. Just comment ot issue, implement it and make PR!","title":"How I added an interactive segmentation feature to CVAT."},{"location":"misc/01_contribution_to_cvat/#how-i-added-an-interactive-segmentation-feature-to-cvat","text":"In this article, I will explain how it's easy to add a semi & auto annotation feature to CVAT. When the function is merged to CVAT, I felt joy because the function might be used by many users. So I really recommend contributing to OSS if you haven't. CVAT is a well developed annotation tool for computer vision tasks. One of the interesting features of CVAT is semi annotation & auto annotation feature. I wanted to use one of the interactive segmentation feature, which helps with annotating instance segmentation, but the features in the CVAT was not enough for my task. So I implemented a new interactive segmentation feature. The screenshot below is the screenshot of using the new feature. The polygon with black line and painted with white half transparent is the estimated segmentation with new semi annotation. The following is the note the way I merge the new feature to CVAT.","title":"How I added an interactive segmentation feature to CVAT."},{"location":"misc/01_contribution_to_cvat/#what-how-i-implemented","text":"I already confirmed the basic functionality of CVAT and tried the annotation a bit. I tried an interactive segmentation following this procedure . I will recommend using them if you haven't used CVAT and want to contribute to them. The semi & automatic annotation is implemented as a server less function and what I need to add is just a serverless function and cvat will add the serverless function automatically to UI. By checking nuclio (serverless function management tool) document, and some config file, I guessed that what I need to write is mostly writing docker file for the container. So I read some config files and serverless function scripts under this directory and environment construction procedure of new interactive segmentation model and just wrote the new config files and serverless function script. The actual PR is here .","title":"What &amp; How I implemented"},{"location":"misc/01_contribution_to_cvat/#read-contribution-guideline","text":"There's a contribution guide for contributors. Usually the repository has a code style guideline for each language in the repository, but there seems not for yaml and python maybe because the code line is not so many. So I just see other code and followed their implicit code style.","title":"Read contribution guideline"},{"location":"misc/01_contribution_to_cvat/#clean-code-update-documentation","text":"I cleaned code and yaml, just rename container name, serverless function name, conform the style to other code, update README, tutorial, and CHANGELOG. That's it.","title":"Clean code &amp; Update documentation"},{"location":"misc/01_contribution_to_cvat/#respond-to-comments-and-fix-it","text":"I didn't get a code fix request and I got a gratitude and code has merged.","title":"Respond to comments and fix it"},{"location":"misc/01_contribution_to_cvat/#takeaway","text":"This time merging PR was really easy & fun. Of course, difficulty of contributing OSS projects depends on project and functions. For example, frontend has a more strict guideline about automated testing and coding styles. So if are not front-end engineer, you should avoid contribute to this part (or maybe you should if you like to learn front-end part). So find issues or feature request from your favorite OSS, check the guideline and if you think you are confident you can contribute to the OSS. Just comment ot issue, implement it and make PR!","title":"Takeaway"},{"location":"misc/02_nuclio_memo/","text":"Strange error while deploying serverless functions to nuclio so many times \u00b6 I encountered an strange error like below while deploying serverless function many times. Call stack: stdout: cat: read error: Is a directory ( abbreviated ) stderr: /nuclio/pkg/cmdrunner/shellrunner.go:96 Failed to execute command: /bin/sh -c \"/bin/cat /etc/nuclio/store/functions/nuclio/*\" /nuclio/pkg/platform/local/store.go:408 Failed to run cat command /nuclio/pkg/platform/local/store.go:335 Failed to get functions /nuclio/pkg/platform/local/store.go:253 Failed to read functions from local store /nuclio/pkg/platform/local/store.go:217 This was solved by running the following command. # Find docker container id of image name alpine:3.11. docker ps -a docker exec ${ docker_container_id } /bin/sh -c \"rm -r /etc/nuclio/store/functions/nuclio/*\"","title":"Strange error while deploying serverless functions to nuclio so many times"},{"location":"misc/02_nuclio_memo/#strange-error-while-deploying-serverless-functions-to-nuclio-so-many-times","text":"I encountered an strange error like below while deploying serverless function many times. Call stack: stdout: cat: read error: Is a directory ( abbreviated ) stderr: /nuclio/pkg/cmdrunner/shellrunner.go:96 Failed to execute command: /bin/sh -c \"/bin/cat /etc/nuclio/store/functions/nuclio/*\" /nuclio/pkg/platform/local/store.go:408 Failed to run cat command /nuclio/pkg/platform/local/store.go:335 Failed to get functions /nuclio/pkg/platform/local/store.go:253 Failed to read functions from local store /nuclio/pkg/platform/local/store.go:217 This was solved by running the following command. # Find docker container id of image name alpine:3.11. docker ps -a docker exec ${ docker_container_id } /bin/sh -c \"rm -r /etc/nuclio/store/functions/nuclio/*\"","title":"Strange error while deploying serverless functions to nuclio so many times"},{"location":"misc/03_start_writing/","text":"Started trying to have a writing habit \u00b6 I started acquiring process of a writing habit. I will write down why and how in this article. This article is unnecessary who don't have any psychological impedance in writing document. Writing habit might be a really good investment \u00b6 The reason of trying to write document frequently is that I'm required to write in many situations, in my work, when contributing to OSS, when writing some blogging services. Imagine your everyday, if you are software engineer, you write comment, document, chat everyday, even your code is like natural language, because the code should be self-descriptive. The advantages of practicing writing are One can be transform a thought into a text faster and all the activity above will be more efficient. One is forced to organize the thought in the process of writing the document. However, I really hate writing, so I decided forcefully imposed myself writing daily. How to continue this habit? \u00b6 Firstly I put a goal, because empirically having a meaningful exciting goal makes you work hard on the goal. You can put any goal you want to achieve by writing, but I put a goal of writing a technical book and writing a paper. Next thing is divide the big goal to smaller goal. The sub goal is writing the 1500 words per a day. This corresponds to 3 pages for a paper. So this will correspond to a one book in 2 months. To achieve 1500 words per day, it is actually a bit hard for the people who hate writing a document. So one trial I did is starting from writing a short notes for a while and write a 1500 words totally. This short note is still meaningful to write because it has two these outcomes It allow me to leave small ideas in more organized way. It is still a practice for writing a note These text can be used as a small text between main chapters. How to practice and grow writing skill? \u00b6 So by writing a document, the another goal of \"writing a better document\" is described more clearly by Use words more precisely to avoid misunderstanding and make readers easier to understand Write document faster by knowing new words, new way of sentence construction, new expressions by getting used to the expressions. Third goal can be achieved by just writing. First and second goal seems needs some other methods to achieve. The method I adopted is that refining the document. My goal is writing a technical book and a paper, so refining process is necessary when trying to integrate it to the book. This process give me an opportunity to learn new expression and correct the wrong usage of the words. More in detail? \u00b6 So how can we do that more in detail? There are multiple ways of doing this but I decided to write a document in markdown format and used vscode as an editor because vscode is my favorite editor and the customization for writing editor is really easier than other editors like emacs. (Emacs was my favorite editor before. I was too lazy maintaining emacs config for different environment) There are extension to write markdown efficiently and even extension to count words. Other tool that can be useful is DeepL. The translation tool is really well translate text and can be used like this. Write sentence in your mother language and translate the sentence to English and check if some new words or expression has appear. Translate sentence in English into your mother language and check if there's strange expression in it. Takeaway \u00b6 Takeaways are To practicing writing document might be a good investment To practice document, write document daily might be useful To achieve habit, setting a goal and divide sub goals to a daily unit might motivate you more. Some type of improvement cycle is necessary to improve skill.","title":"Started trying to have a writing habit"},{"location":"misc/03_start_writing/#started-trying-to-have-a-writing-habit","text":"I started acquiring process of a writing habit. I will write down why and how in this article. This article is unnecessary who don't have any psychological impedance in writing document.","title":"Started trying to have a writing habit"},{"location":"misc/03_start_writing/#writing-habit-might-be-a-really-good-investment","text":"The reason of trying to write document frequently is that I'm required to write in many situations, in my work, when contributing to OSS, when writing some blogging services. Imagine your everyday, if you are software engineer, you write comment, document, chat everyday, even your code is like natural language, because the code should be self-descriptive. The advantages of practicing writing are One can be transform a thought into a text faster and all the activity above will be more efficient. One is forced to organize the thought in the process of writing the document. However, I really hate writing, so I decided forcefully imposed myself writing daily.","title":"Writing habit might be a really good investment"},{"location":"misc/03_start_writing/#how-to-continue-this-habit","text":"Firstly I put a goal, because empirically having a meaningful exciting goal makes you work hard on the goal. You can put any goal you want to achieve by writing, but I put a goal of writing a technical book and writing a paper. Next thing is divide the big goal to smaller goal. The sub goal is writing the 1500 words per a day. This corresponds to 3 pages for a paper. So this will correspond to a one book in 2 months. To achieve 1500 words per day, it is actually a bit hard for the people who hate writing a document. So one trial I did is starting from writing a short notes for a while and write a 1500 words totally. This short note is still meaningful to write because it has two these outcomes It allow me to leave small ideas in more organized way. It is still a practice for writing a note These text can be used as a small text between main chapters.","title":"How to continue this habit?"},{"location":"misc/03_start_writing/#how-to-practice-and-grow-writing-skill","text":"So by writing a document, the another goal of \"writing a better document\" is described more clearly by Use words more precisely to avoid misunderstanding and make readers easier to understand Write document faster by knowing new words, new way of sentence construction, new expressions by getting used to the expressions. Third goal can be achieved by just writing. First and second goal seems needs some other methods to achieve. The method I adopted is that refining the document. My goal is writing a technical book and a paper, so refining process is necessary when trying to integrate it to the book. This process give me an opportunity to learn new expression and correct the wrong usage of the words.","title":"How to practice and grow writing skill?"},{"location":"misc/03_start_writing/#more-in-detail","text":"So how can we do that more in detail? There are multiple ways of doing this but I decided to write a document in markdown format and used vscode as an editor because vscode is my favorite editor and the customization for writing editor is really easier than other editors like emacs. (Emacs was my favorite editor before. I was too lazy maintaining emacs config for different environment) There are extension to write markdown efficiently and even extension to count words. Other tool that can be useful is DeepL. The translation tool is really well translate text and can be used like this. Write sentence in your mother language and translate the sentence to English and check if some new words or expression has appear. Translate sentence in English into your mother language and check if there's strange expression in it.","title":"More in detail?"},{"location":"misc/03_start_writing/#takeaway","text":"Takeaways are To practicing writing document might be a good investment To practice document, write document daily might be useful To achieve habit, setting a goal and divide sub goals to a daily unit might motivate you more. Some type of improvement cycle is necessary to improve skill.","title":"Takeaway"},{"location":"misc/04_check_oss_for_contribution/","text":"Check open source softwares for contributions \u00b6 Contributing open source software seems interesting. I read contribution guides from OSS I have some interest. OpenCV \u00b6 Contribution to OpenCV \u00b6 OpenCV is a computer vision library that has many features. I think most of the software engineer who is related to computer vision has used this library. Since there are ton's of features, I guess this software can be a candidate for many software engineer who touches computer vision including me. As one of the most popular OSS software, contributor has many option. The contributor is Review new code or join discussion on new code Bug fix feature request Test the latest code Optimization for different platforms Which task to do heavily depends on your preference and personality. I don't have a specific feature to develop yet so I decided to start from 1, 2, 4 to familiarize myself to OpenCV. Find something to contribute \u00b6 First step will be check pull-requests, issues. Testing the latest code is interesting, but not sure there's a bug to report and unsure if I can contribute to them by this way. I read some closed PR is I found out that OpenCV reviewers and contributors are really polite and review is good atmosphere. It seems there's no big room to do a discussion. I think both making a pull-request and reviewing the code might be fun, but it's a bit difficult to find the PRs that I can review with my skills. Next thing I checked issues, firstly I checked the issues with bug label but it's a bit hard to find the ones I can solve. So next I checked the issues with feature labels. It's a bit easier to find the tickets to solve. One of the dnn module that can be used as a runtime of the dnn model looks attracting. So this is the candidate for this. DNN Runtimes \u00b6 There are many DNN runtimes and it worth surveying these DNN runtimes. There are some tensorflow supports running tflite model (this is a lighter model) onnx model has many features, many converter, support for optimization, running on different dnn runtime openvino is optimized for intel products Among them, onnx is the most attractive product to use & contribute. ONNX Runtime \u00b6 ONNX runtime is has many features. It supports many programming language bindings like C++, Python,...etc It supports many execution engine like openvino, TensorRT. There are many converter between onnx and other model formats. So this is really attractive product. To contribute to this project for the first time, they recommend resolving issues with \"good first issue\" tag, but there is not... So it seems that I need to find myself appropriate issues for myself. DNN Training Frameworks \u00b6 pytorch, tensorflow, etc... I think it is a bit difficult for me to contribute to such frameworks. Scientific calculation libraries \u00b6 scipy, numpy, etc... I think it is a bit difficult for me to contribute to such frameworks. But probably depend on the feature. So probably they can be contribution target. Higher level DNN Frameworks \u00b6 something like mmdetection. It might be a target. After searching OSS contribution target... \u00b6 After checking issues and features of many OSSs, I realized that OSS has many features, and issues realated to them, issues are different level from lower level to higher level. So I think I should check issues daily and try to resolve them if I'm interested in one of them. And continuously learn features of the framework and add features to them when I come up with it. These are the way to start contributing to OSS.","title":"Check open source softwares for contributions"},{"location":"misc/04_check_oss_for_contribution/#check-open-source-softwares-for-contributions","text":"Contributing open source software seems interesting. I read contribution guides from OSS I have some interest.","title":"Check open source softwares for contributions"},{"location":"misc/04_check_oss_for_contribution/#opencv","text":"","title":"OpenCV"},{"location":"misc/04_check_oss_for_contribution/#contribution-to-opencv","text":"OpenCV is a computer vision library that has many features. I think most of the software engineer who is related to computer vision has used this library. Since there are ton's of features, I guess this software can be a candidate for many software engineer who touches computer vision including me. As one of the most popular OSS software, contributor has many option. The contributor is Review new code or join discussion on new code Bug fix feature request Test the latest code Optimization for different platforms Which task to do heavily depends on your preference and personality. I don't have a specific feature to develop yet so I decided to start from 1, 2, 4 to familiarize myself to OpenCV.","title":"Contribution to OpenCV"},{"location":"misc/04_check_oss_for_contribution/#find-something-to-contribute","text":"First step will be check pull-requests, issues. Testing the latest code is interesting, but not sure there's a bug to report and unsure if I can contribute to them by this way. I read some closed PR is I found out that OpenCV reviewers and contributors are really polite and review is good atmosphere. It seems there's no big room to do a discussion. I think both making a pull-request and reviewing the code might be fun, but it's a bit difficult to find the PRs that I can review with my skills. Next thing I checked issues, firstly I checked the issues with bug label but it's a bit hard to find the ones I can solve. So next I checked the issues with feature labels. It's a bit easier to find the tickets to solve. One of the dnn module that can be used as a runtime of the dnn model looks attracting. So this is the candidate for this.","title":"Find something to contribute"},{"location":"misc/04_check_oss_for_contribution/#dnn-runtimes","text":"There are many DNN runtimes and it worth surveying these DNN runtimes. There are some tensorflow supports running tflite model (this is a lighter model) onnx model has many features, many converter, support for optimization, running on different dnn runtime openvino is optimized for intel products Among them, onnx is the most attractive product to use & contribute.","title":"DNN Runtimes"},{"location":"misc/04_check_oss_for_contribution/#onnx-runtime","text":"ONNX runtime is has many features. It supports many programming language bindings like C++, Python,...etc It supports many execution engine like openvino, TensorRT. There are many converter between onnx and other model formats. So this is really attractive product. To contribute to this project for the first time, they recommend resolving issues with \"good first issue\" tag, but there is not... So it seems that I need to find myself appropriate issues for myself.","title":"ONNX Runtime"},{"location":"misc/04_check_oss_for_contribution/#dnn-training-frameworks","text":"pytorch, tensorflow, etc... I think it is a bit difficult for me to contribute to such frameworks.","title":"DNN Training Frameworks"},{"location":"misc/04_check_oss_for_contribution/#scientific-calculation-libraries","text":"scipy, numpy, etc... I think it is a bit difficult for me to contribute to such frameworks. But probably depend on the feature. So probably they can be contribution target.","title":"Scientific calculation libraries"},{"location":"misc/04_check_oss_for_contribution/#higher-level-dnn-frameworks","text":"something like mmdetection. It might be a target.","title":"Higher level DNN Frameworks"},{"location":"misc/04_check_oss_for_contribution/#after-searching-oss-contribution-target","text":"After checking issues and features of many OSSs, I realized that OSS has many features, and issues realated to them, issues are different level from lower level to higher level. So I think I should check issues daily and try to resolve them if I'm interested in one of them. And continuously learn features of the framework and add features to them when I come up with it. These are the way to start contributing to OSS.","title":"After searching OSS contribution target..."},{"location":"misc/05_wake_word_detection/","text":"Alternatives for wake word detection for any language \u00b6 I'm implementing an AI assistant software. And I need to implement a wake word detection feature. A wake word detection is the algorithm to detect some word to \"wake\" AI. That means the wake word is something like command to trigger some process. The wake word is usually short sentence like, \"Alexa\", or \"Hey siri\" and the role is distinguish the human voice from other sounds and distinguish the wake word from other words. The most famous wake word detection method is probably \"snowboy\", but the development has stopped and the website for constructing model is closed. So I tried to find an another way to construct a wake word detection. Feature Extractor for audio \u00b6 There are two pretrained model to extract embeddings from audio. One is YAMNet , this model is classification model of the audio, but it can also output embedding from the audio. From this list of output classes , I guess that this class can separate different kind of sounds, probably the distinguishable sound is human voice from cat voice, or human voice from other voice. or not. There are also more detailed classes for human voices, but I guess it cannot distinguish different word because from the classes the model is not trained to do so. Another feature extractor is FRILL , FRILL is the best speech recognition feature extractor for non semantic speech recognition task. The examples of non semantic speech recognition task is speech emotion recognition, language identification, age recognition. The semantic speech recognition task is automatic speech recognition, phone classification. So I guess recognition based on word is very challenging or impossible with this feature extractor. So what should I do? \u00b6 I redesign the application I'm developing. Firstly detect the human voice and apply SpeechToText and check if it's a wake word or not . The dataset that is used to train FRILL and YAMNet has these classes for human speech and \"Speech\" class can be used for this. Since YAMNet is already a classifier, it's easier to use than FRILL. In my case, YAMNet is easiest one so I started using the model. Implementation of human voice detector \u00b6 Here is the implementation. Basically the script is like this Read sound from the microphone for 0.1 Put them into circular buffer When there is a enough 0.3[s] buffer, do inference on the buffered sound. When listen human voice, keep accumulating sound until human voice is over I feel recognition accuracy is so-so. Also the table below is the computational costs of this model of onnx and original model. Runtime Mean [ms] Std [ms] Video Memory [MiB] tensorflow-gpu 4.879195771055545 0.47763425563967565 6013MiB onnxruntime-gpu (CPUExecutionProvider) 1.2899305888854196 0.23154279587315799 0 onnxruntime-gpu (CUDAExecutionProvider) 14.181141914335141 2.326298014955945 1187 It seems that the CUDAExecutionProvider of the onnxruntime-gpu is slower than the tensorflow-gpu. And the fastest one is CPU on onnxruntime. The onnx model is converted from the tflite version of this model. but I got a similar result when converting the model from saved_model.pb. Even when trying different ops versions (both 14 and 13). My expectation was both gpu is faster on onnx and cpu was not sure because tf2onnx optimizes the model when converting the model into onnx. So far CPU seems the best results. Also it can be redesigned like, after detecting human voice wave, the speech to text can be run on the voice wave and detected text can be compared with another voice. Implementation of SpeechToText function \u00b6 This implementation is extremely easy. You can use ESPNet to do speech to text or text to speech for various language. Just follow this article , and you can implement speech to text algorithm of Japanese. Takeaway \u00b6 Even though it's not super power efficient and computational efficiently that can be used in embedding systems. But the combination of the human voice detection and speech to text approach works better than wake word detection, without training data for wake word. This is really useful for prototyping or not computational scarce environment.","title":"Alternatives for wake word detection for any language"},{"location":"misc/05_wake_word_detection/#alternatives-for-wake-word-detection-for-any-language","text":"I'm implementing an AI assistant software. And I need to implement a wake word detection feature. A wake word detection is the algorithm to detect some word to \"wake\" AI. That means the wake word is something like command to trigger some process. The wake word is usually short sentence like, \"Alexa\", or \"Hey siri\" and the role is distinguish the human voice from other sounds and distinguish the wake word from other words. The most famous wake word detection method is probably \"snowboy\", but the development has stopped and the website for constructing model is closed. So I tried to find an another way to construct a wake word detection.","title":"Alternatives for wake word detection for any language"},{"location":"misc/05_wake_word_detection/#feature-extractor-for-audio","text":"There are two pretrained model to extract embeddings from audio. One is YAMNet , this model is classification model of the audio, but it can also output embedding from the audio. From this list of output classes , I guess that this class can separate different kind of sounds, probably the distinguishable sound is human voice from cat voice, or human voice from other voice. or not. There are also more detailed classes for human voices, but I guess it cannot distinguish different word because from the classes the model is not trained to do so. Another feature extractor is FRILL , FRILL is the best speech recognition feature extractor for non semantic speech recognition task. The examples of non semantic speech recognition task is speech emotion recognition, language identification, age recognition. The semantic speech recognition task is automatic speech recognition, phone classification. So I guess recognition based on word is very challenging or impossible with this feature extractor.","title":"Feature Extractor for audio"},{"location":"misc/05_wake_word_detection/#so-what-should-i-do","text":"I redesign the application I'm developing. Firstly detect the human voice and apply SpeechToText and check if it's a wake word or not . The dataset that is used to train FRILL and YAMNet has these classes for human speech and \"Speech\" class can be used for this. Since YAMNet is already a classifier, it's easier to use than FRILL. In my case, YAMNet is easiest one so I started using the model.","title":"So what should I do?"},{"location":"misc/05_wake_word_detection/#implementation-of-human-voice-detector","text":"Here is the implementation. Basically the script is like this Read sound from the microphone for 0.1 Put them into circular buffer When there is a enough 0.3[s] buffer, do inference on the buffered sound. When listen human voice, keep accumulating sound until human voice is over I feel recognition accuracy is so-so. Also the table below is the computational costs of this model of onnx and original model. Runtime Mean [ms] Std [ms] Video Memory [MiB] tensorflow-gpu 4.879195771055545 0.47763425563967565 6013MiB onnxruntime-gpu (CPUExecutionProvider) 1.2899305888854196 0.23154279587315799 0 onnxruntime-gpu (CUDAExecutionProvider) 14.181141914335141 2.326298014955945 1187 It seems that the CUDAExecutionProvider of the onnxruntime-gpu is slower than the tensorflow-gpu. And the fastest one is CPU on onnxruntime. The onnx model is converted from the tflite version of this model. but I got a similar result when converting the model from saved_model.pb. Even when trying different ops versions (both 14 and 13). My expectation was both gpu is faster on onnx and cpu was not sure because tf2onnx optimizes the model when converting the model into onnx. So far CPU seems the best results. Also it can be redesigned like, after detecting human voice wave, the speech to text can be run on the voice wave and detected text can be compared with another voice.","title":"Implementation of human voice detector"},{"location":"misc/05_wake_word_detection/#implementation-of-speechtotext-function","text":"This implementation is extremely easy. You can use ESPNet to do speech to text or text to speech for various language. Just follow this article , and you can implement speech to text algorithm of Japanese.","title":"Implementation of SpeechToText function"},{"location":"misc/05_wake_word_detection/#takeaway","text":"Even though it's not super power efficient and computational efficiently that can be used in embedding systems. But the combination of the human voice detection and speech to text approach works better than wake word detection, without training data for wake word. This is really useful for prototyping or not computational scarce environment.","title":"Takeaway"},{"location":"misc/06_performance_check/","text":"","title":"06 performance check"}]}